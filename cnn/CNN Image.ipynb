{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4e98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from sgan.data.loader import data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc6638b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0703a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, obs_len=12, pred_len=8, delim=\"tab\", skip=1, metric=\"meter\", batch_size=16, loader_num_workers=1):\n",
    "        self.obs_len = obs_len\n",
    "        self.pred_len = pred_len\n",
    "        self.delim = delim\n",
    "        self.skip = skip\n",
    "        self.metric = metric\n",
    "        self.batch_size = batch_size\n",
    "        self.loader_num_workers = loader_num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b86cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [01:10<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "train_path = \"/mnt/c/Users/ktwzj/code/data/nba/nba-cross.s025/train_sample\"\n",
    "train_path = \"/scratch/gw2145/data/nba/nba-cross.s025/train_sample\"\n",
    "args = Config()\n",
    "train_dset, train_loader = data_loader(args, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fed0e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 176, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    obs_traj = batch[0]\n",
    "    print(obs_traj.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_width = 28.7\n",
    "field_height = 15.2\n",
    "scene = obs_traj[:, :11, :]\n",
    "sampling_scale_factor = 50\n",
    "sampling_resolution = (int(sampling_scale_factor * field_width), int(sampling_scale_factor * field_height) )\n",
    "image_grid = np.zeros((11, sampling_resolution[1], sampling_resolution[0]))\n",
    "image = np.zeros((sampling_resolution[1], sampling_resolution[0]), np.uint8)\n",
    "def build_map_function(scale_factor):\n",
    "    def f (x, y):\n",
    "        return x * scale_factor, y * scale_factor\n",
    "    return f\n",
    "sampling_map_func = build_map_function(sampling_scale_factor)\n",
    "# image = cv2.fromarray(image)\n",
    "image\n",
    "center_coordinates = (129, 30)\n",
    " \n",
    "# Radius of circle\n",
    "radius = 3\n",
    "  \n",
    "# Blue color in BGR\n",
    "color = 128\n",
    "  \n",
    "# Line thickness of 2 px\n",
    "thickness = -1\n",
    "  \n",
    "# Using cv2.circle() method\n",
    "# Draw a circle with blue line borders of thickness of 2 px\n",
    "agent = scene[:, 0, :]\n",
    "for i in range(agent.shape[0]):\n",
    "    x = agent[i,0].item()\n",
    "    y = agent[i, 1].item()\n",
    "#     print(x.item())\n",
    "    x,y = sampling_map_func(x,y)\n",
    "    center_coordinates = (int(x), int(y))\n",
    "    image = cv2.circle(image, center_coordinates, radius, color, thickness)\n",
    "blur = cv2.GaussianBlur(image,(5,5),0)\n",
    "blur = cv2.GaussianBlur(blur,(5,5),0)\n",
    "blur = cv2.GaussianBlur(blur,(5,5),0)\n",
    "\n",
    "blur = cv2.GaussianBlur(blur,(5,5),0)\n",
    "blur = cv2.GaussianBlur(blur,(5,5),0)\n",
    "blur = cv2.GaussianBlur(blur,(5,5),0)\n",
    "blur = cv2.GaussianBlur(blur,(5,5),0)\n",
    "blur = cv2.GaussianBlur(blur,(5,5),0)\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "\n",
    "dilation = cv2.dilate(blur,kernel,iterations = 5)\n",
    "\n",
    "plt.imshow(blur, cmap=\"gray\")\n",
    "\n",
    "plt.subplot(131),plt.imshow(image,cmap=\"gray\"),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(132),plt.imshow(blur, cmap=\"gray\"),plt.title('Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(133),plt.imshow(dilation, cmap=\"gray\"),plt.title('Dilation')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# plt.show()\n",
    "dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1c3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrajectoryDrawer:\n",
    "    field_width = 28.7\n",
    "    field_height = 15.2\n",
    "    def __init__(self, sampling_scale_factor=50, target_size=[572, 572],drawing_mode=\"blur\"):\n",
    "        self.sampling_scale_factor = sampling_scale_factor\n",
    "        self.drawing_mode=drawing_mode\n",
    "        self.target_size = target_size\n",
    "        self.agent_num = 11\n",
    "        self.sampling_resolution = (int(sampling_scale_factor * TrajectoryDrawer.field_width), int(sampling_scale_factor * TrajectoryDrawer.field_height) )\n",
    "\n",
    "    def transform_sampling_coords (self, x, y):\n",
    "        return x * self.sampling_scale_factor, y * self.sampling_scale_factor\n",
    "    \n",
    "    def generate_trajectory_image(self, agent, image):\n",
    "        for i in range(agent.size(0)):\n",
    "            x = agent[i,0].item()\n",
    "            y = agent[i, 1].item()\n",
    "            x,y = self.transform_sampling_coords(x,y)\n",
    "            center_coordinates = (int(x), int(y))\n",
    "            radius = 3\n",
    "            color = 128\n",
    "            thickness = -1\n",
    "            image = cv2.circle(image, center_coordinates, radius, color, thickness)\n",
    "\n",
    "            image = cv2.GaussianBlur(image,(5,5),0)\n",
    "            image = cv2.GaussianBlur(image,(5,5),0)\n",
    "            image = cv2.GaussianBlur(image,(5,5),0)\n",
    "            image = cv2.GaussianBlur(image,(5,5),0)\n",
    "            image = cv2.GaussianBlur(image,(5,5),0)\n",
    "            image = cv2.GaussianBlur(image,(5,5),0)\n",
    "            kernel = np.ones((3,3),np.uint8)\n",
    "            resized = cv2.resize(image,self.target_size, interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "        return resized\n",
    "\n",
    "    def generate_scene_image(self, scene):  \n",
    "        channels = []\n",
    "        for i in range(scene.size(1)):\n",
    "            agent_image = np.zeros((self.sampling_resolution[1], self.sampling_resolution[0]), np.uint8)\n",
    "            agent = scene[:, i, :]\n",
    "            channel = self.generate_trajectory_image(agent, agent_image)\n",
    "            channel = np.array(channel)\n",
    "            channels.append(channel)\n",
    "        scene_image = np.stack(channels, axis=2)\n",
    "        return scene_image\n",
    "    \n",
    "    def generate_batch_images(self, traj_batch):\n",
    "        batch_size = traj_batch.size(1) // self.agent_num\n",
    "        batch_images = []\n",
    "        for i in range(batch_size):\n",
    "            scene = traj_batch[:, i*self.agent_num: (i+1) * self.agent_num, :]\n",
    "            scene_image = self.generate_scene_image(scene)\n",
    "            batch_images.append(scene_image)\n",
    "        batch_images = np.stack(batch_images)\n",
    "        batch_images = torch.Tensor(batch_images)\n",
    "        batch_images = batch_images.permute(0, 3, 1, 2)\n",
    "        return batch_images\n",
    "# drawer = TrajectoryDrawer()\n",
    "# image = drawer.generate_batch_images(obs_traj)\n",
    "# image.size()\n",
    "# image.shape\n",
    "# plt.imshow(image[:,:,:1], cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9344e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class LNet(nn.Module):\n",
    "    def __init__(self, n_channels, bilinear=True):\n",
    "        super(LNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        return x5\n",
    " \n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3a076edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Query: Image\n",
    "    Key-Value: Hidden States\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dim, hidden_dim, n_head=16, key_dim=16, value_dim=16, dropout=0.5):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        \n",
    "        self.image_dim = image_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.n_head = n_head\n",
    "        self.w_qs = nn.Linear(hidden_dim, n_head * key_dim, bias=False)\n",
    "        self.w_ks = nn.Linear(image_dim, n_head * key_dim, bias=False)\n",
    "        self.w_vs = nn.Linear(image_dim, n_head * value_dim, bias=False)\n",
    "        self.fc = nn.Linear(n_head * value_dim, hidden_dim, bias=False)\n",
    "        self.dot_attention = ScaledDotProductAttention(temperature=key_dim ** 0.5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "\n",
    "    \n",
    "    def forward(self, images, hiddens):\n",
    "        n_head = self.n_head\n",
    "        key_dim = self.key_dim\n",
    "        value_dim = self.value_dim\n",
    "        batch_size = images.size(0)\n",
    "        image_dim =images.size(1)\n",
    "        k = images.view(batch_size, image_dim, -1)\n",
    "        k = k.permute(0, 2, 1)\n",
    "        len_image = k.size(1)\n",
    "        \n",
    "        q = hiddens\n",
    "        residual = q\n",
    "        len_hidden = 11\n",
    "        v = k\n",
    "        residual = hiddens\n",
    "        q = self.w_qs(q).view(batch_size, len_hidden, n_head, key_dim)\n",
    "        k = self.w_ks(k).view(batch_size, len_image, n_head, key_dim)\n",
    "        v = self.w_vs(v).view(batch_size, len_image, n_head, value_dim)\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "        q, attn = self.dot_attention(q, k, v)\n",
    "        q = q.transpose(1, 2).contiguous().view(batch_size, len_hidden, -1)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "        return q\n",
    "\n",
    "\n",
    "class PostAttentionFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in=32, d_hid=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n",
    "        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ImageAttentionLayer(nn.Module):\n",
    "    ''' Compose with two layers '''\n",
    "\n",
    "    def __init__(self, image_dim, hidden_dim, d_inner=128, n_head=16, key_dim=16, value_dim=16, dropout=0.5):\n",
    "        super(ImageAttentionLayer, self).__init__()\n",
    "        self.attn = AttentionModule(image_dim=image_dim, hidden_dim=hidden_dim, n_head=n_head, key_dim=key_dim, value_dim=value_dim, dropout=dropout)\n",
    "        self.post_ffn = PostAttentionFeedForward(hidden_dim, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, images, hiddens):\n",
    "        attn_output = self.attn(\n",
    "            images, hiddens)\n",
    "        layer_output = self.post_ffn(attn_output)\n",
    "        return  images, layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9f53a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from sgan.models import Encoder, Decoder, get_noise\n",
    "class TrajectoryModel(nn.Module):\n",
    "    def __init__(self, pred_len=8, encoder_h_dim=32, decoder_h_dim = 32, noise_dim=(0, ), noise_type='gaussian', noise_mix_type='ped'):\n",
    "        super(TrajectoryModel, self).__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.image_drawer = TrajectoryDrawer(target_size=[224, 224])\n",
    "        \n",
    "        self.image_feature_extractor = self._make_image_feature_extractor()\n",
    "        self.encoder = Encoder(h_dim=encoder_h_dim)\n",
    "        self.num_layers = 1\n",
    "        self.decoder_h_dim = decoder_h_dim\n",
    "        self.encoder_h_dim = encoder_h_dim\n",
    "        self.noise_mix_type = noise_mix_type\n",
    "        self.noise_dim = noise_dim\n",
    "        self.noise_type = noise_type\n",
    "        \n",
    "        self.attentions = nn.ModuleList([ImageAttentionLayer(hidden_dim=encoder_h_dim, image_dim=512) for i in range(2)])\n",
    "        self.to_spatial = nn.Linear(encoder_h_dim, 2 * pred_len)\n",
    "#         self.decoder = Decoder(\n",
    "#             pred_len,\n",
    "#             embedding_dim=embedding_dim,\n",
    "#             h_dim=decoder_h_dim,\n",
    "#             mlp_dim=mlp_dim,\n",
    "#             num_layers=num_layers,\n",
    "#             pool_every_timestep=pool_every_timestep,\n",
    "#             dropout=dropout,\n",
    "#             bottleneck_dim=bottleneck_dim,\n",
    "#             activation=activation,\n",
    "#             batch_norm=batch_norm,\n",
    "#             pooling_type=pooling_type,\n",
    "#             grid_size=grid_size,\n",
    "#             neighborhood_size=neighborhood_size,)\n",
    "    def _make_image_feature_extractor(self):\n",
    "        resnet = models.resnet18()\n",
    "        conv1 = nn.Conv2d(11, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        module_list = list(resnet.children())\n",
    "        image_feature_extractor = nn.Sequential(conv1, *module_list[1:-2])\n",
    "        \n",
    "        return image_feature_extractor\n",
    "    def add_noise(self, _input, seq_start_end, user_noise=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - _input: Tensor of shape (_, decoder_h_dim - noise_first_dim)\n",
    "        - seq_start_end: A list of tuples which delimit sequences within batch.\n",
    "        - user_noise: Generally used for inference when you want to see\n",
    "        relation between different types of noise and outputs.\n",
    "        Outputs:\n",
    "        - decoder_h: Tensor of shape (_, decoder_h_dim)\n",
    "        \"\"\"\n",
    "        if not self.noise_dim:\n",
    "            return _input\n",
    "\n",
    "        if self.noise_mix_type == 'global':\n",
    "            noise_shape = (seq_start_end.size(0), ) + self.noise_dim\n",
    "        else:\n",
    "            noise_shape = (_input.size(0), ) + self.noise_dim\n",
    "\n",
    "        if user_noise is not None:\n",
    "            z_decoder = user_noise\n",
    "        else:\n",
    "            z_decoder = get_noise(noise_shape, self.noise_type)\n",
    "\n",
    "        if self.noise_mix_type == 'global':\n",
    "            _list = []\n",
    "            for idx, (start, end) in enumerate(seq_start_end):\n",
    "                start = start.item()\n",
    "                end = end.item()\n",
    "                _vec = z_decoder[idx].view(1, -1)\n",
    "                _to_cat = _vec.repeat(end - start, 1)\n",
    "                _list.append(torch.cat([_input[start:end], _to_cat], dim=1))\n",
    "            decoder_h = torch.cat(_list, dim=0)\n",
    "            return decoder_h\n",
    "\n",
    "        decoder_h = torch.cat([_input, z_decoder], dim=1)\n",
    "\n",
    "        return decoder_h\n",
    "    \n",
    "    \n",
    "    def forward(self, obs_traj, obs_traj_rel, seq_start_end, obs_team, obs_pos, user_noise=None,):\n",
    "        batch_size = obs_traj_rel.size(1) // 11\n",
    "        images = self.image_drawer.generate_batch_images(obs_traj.cpu())\n",
    "        images = images.cuda()\n",
    "        images.requires_grad=False\n",
    "        \n",
    "        image_features = self.image_feature_extractor(images)\n",
    "        image_features = torch.squeeze(image_features)\n",
    "        final_encoder_h = self.encoder(obs_traj_rel)\n",
    "        hiddens = torch.squeeze(final_encoder_h)\n",
    "        hiddens = hiddens.view(batch_size, 11, -1)\n",
    "        \n",
    "        \n",
    "        for attention_layer in self.attentions:\n",
    "            _, h = attention_layer(image_features, hiddens)\n",
    "        spatial = self.to_spatial(h).view(batch_size * 11, -1)\n",
    "        spatial = spatial.view(-1, self.pred_len, 2)\n",
    "        spatial = spatial.permute(1, 0, 2)\n",
    "#         mlp_decoder_context_input = final_encoder_h.view(\n",
    "#                 -1, self.encoder_h_dim)\n",
    "#         noise_input = mlp_decoder_context_input\n",
    "#         decoder_h = self.add_noise(\n",
    "#             noise_input, seq_start_end, user_noise=user_noise)\n",
    "        \n",
    "#         decoder_h = torch.unsqueeze(decoder_h, 0)\n",
    "#         decoder_c = torch.zeros(\n",
    "#             self.num_layers, batch_size, self.decoder_h_dim\n",
    "#         ).cuda()\n",
    "\n",
    "#         state_tuple = (decoder_h, decoder_c)\n",
    "#         last_pos = obs_traj[-1]\n",
    "#         last_pos_rel = obs_traj_rel[-1]\n",
    "#         # Predict Trajectory\n",
    "\n",
    "#         decoder_out = self.decoder(\n",
    "#             last_pos,\n",
    "#             last_pos_rel,\n",
    "#             state_tuple,\n",
    "#             seq_start_end,\n",
    "#         )\n",
    "        \n",
    "        return spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2322eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [tensor.cuda() for tensor in batch]\n",
    "(obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, obs_team_vec, obs_pos_vec, pred_team_vec, pred_pos_vec, non_linear_ped, loss_mask, seq_start_end) = batch\n",
    "drawer = TrajectoryDrawer(target_size=[224, 224])\n",
    "images = drawer.generate_batch_images(obs_traj)\n",
    "images = images.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ad49a167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 176, 2])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.empty_cache()\n",
    "model = TrajectoryModel()\n",
    "model = model.cuda()\n",
    "r = model(obs_traj, obs_traj_rel, seq_start_end, obs_team_vec, obs_pos_vec)\n",
    "r.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eab10b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "176/ 16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
